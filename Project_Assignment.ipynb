{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "bRdJ0_o2BCpK",
        "MCt9pJZvEkfw",
        "IzpteW1JEr4B",
        "UF4vaIhvMqHu",
        "rQgtH-faBiGW"
      ],
      "authorship_tag": "ABX9TyNP+yP+tQFOIFsiyqi5aUHh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajprasad001/deep_learning_concepts/blob/master/Project_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXwF52iXMccG",
        "colab_type": "text"
      },
      "source": [
        "# Project Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D93JAy_dVSg",
        "colab_type": "text"
      },
      "source": [
        " ## Importing Dependencies(Libraries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAK8-A7tHBdo",
        "colab_type": "text"
      },
      "source": [
        "Major Libraries Used : TensorFlow, Keras, scikit-learn, matplotlib, NumPy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzT8J9WydjQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import cifar10\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRdJ0_o2BCpK",
        "colab_type": "text"
      },
      "source": [
        "## Loading and Splitting Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BqVrmOwGNBd",
        "colab_type": "text"
      },
      "source": [
        "We will be using 2 different dataset for the completion of the assignments\n",
        "1. CIFAR10\n",
        "2. Fashion MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCt9pJZvEkfw",
        "colab_type": "text"
      },
      "source": [
        "### Loading and Splitting CIFAR10 DATA into Train, Validation and Test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqK5A4psBHL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Splitting the data into Train and Test. The Test Data should be completely unseen.\n",
        "(cifar_train_x_temp, cifar_train_y_temp), (cifar_test_x , cifar_test_y) =cifar10.load_data() \n",
        "#By default 10000 instances are used as test data in Cifar10\n",
        "\n",
        "cifar_train_x, cifar_val_x, cifar_train_y, cifar_val_y = train_test_split(cifar_train_x_temp, cifar_train_y_temp, test_size=0.20, random_state=42)\n",
        "#Further, for hyperparameter tuning, 20% of the train data is futher split into train and validation data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89fRmsecCqYP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e9eba86f-9088-44fb-b739-960fac5f7ee8"
      },
      "source": [
        "print('cifar10 train data      : {}'.format(cifar_train_x.shape))\n",
        "print('cifar10 validation data : {}'.format(cifar_val_x.shape))\n",
        "print('cifar10 test data       : {}'.format(cifar_test_x.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cifar10 train data      : (40000, 32, 32, 3)\n",
            "cifar10 validation data : (10000, 32, 32, 3)\n",
            "cifar10 test data       : (10000, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nncS5aMfJCpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Standardizing the train dataset\n",
        "cifar_train_y = cifar_train_y.reshape((-1))\n",
        "cifar_train_x = tf.data.Dataset.from_tensor_slices((cifar_train_x.reshape([-1,32,32,3]).astype(np.float32)/255, cifar_train_y.astype(np.int32)))\n",
        "\n",
        "#Standardizing the validation dataset\n",
        "cifar_val_y = cifar_val_y.reshape((-1))\n",
        "cifar_val_x = tf.data.Dataset.from_tensor_slices((cifar_val_x.reshape([-1,32,32,3]).astype(np.float32)/255, cifar_val_y.astype(np.int32)))\n",
        "\n",
        "#Standardizing the test dataset\n",
        "cifar_test_y = cifar_test_y.reshape((-1))\n",
        "cifar_test_x = tf.data.Dataset.from_tensor_slices((cifar_test_x.reshape([-1,32,32,3]).astype(np.float32)/255, cifar_test_y.astype(np.int32)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzpteW1JEr4B",
        "colab_type": "text"
      },
      "source": [
        "### Loading and Splitting MNIST FASHION DATA into Train, Validation and Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN5-zJyOE4K5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "fef128a0-11fb-48ef-c0d7-641242deaedd"
      },
      "source": [
        "(mnist_train_x_temp, mnist_train_y_temp), (mnist_test_x , mnist_test_y) =fashion_mnist.load_data() \n",
        "#Splitting the data into Train and Test. The Test Data should be completely unseen.\n",
        "#By default 10000 instances are used as test data in Fashion Mnist\n",
        "mnist_train_x, mnist_val_x, mnist_train_y, mnist_val_y = train_test_split(mnist_train_x_temp, mnist_train_y_temp, test_size=0.20, random_state=42)\n",
        "#Further, for hyperparameter tuning, 20% of the train data is futher split into train and validation data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfJAceOuFr4e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b6f07bd0-6141-494a-ada9-d28bf15732a5"
      },
      "source": [
        "print('MNIST FASHION train data      : {}'.format(mnist_train_x.shape))\n",
        "print('MNIST FASHION validation data : {}'.format(mnist_val_x.shape))\n",
        "print('MNIST FASHION test data       : {}'.format(mnist_test_x.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MNIST FASHION train data      : (48000, 28, 28)\n",
            "MNIST FASHION validation data : (12000, 28, 28)\n",
            "MNIST FASHION test data       : (10000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5rO7CHPJPUS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "35d42271-99c7-4c8b-ff8e-e146f9d5444b"
      },
      "source": [
        "#Standardizing the train dataset\n",
        "mnist_train_y = mnist_train_y.reshape((-1))\n",
        "cifar_train_x = tf.data.Dataset.from_tensor_slices((mnist_train_x.reshape([-1,32,32,3]).astype(np.float32)/255, mnist_train_y.astype(np.int32)))\n",
        "\n",
        "#Standardizing the validation dataset\n",
        "mnist_val_y = mnist_val_y.reshape((-1))\n",
        "mnist_val_x = tf.data.Dataset.from_tensor_slices((mnist_val_x.reshape([-1,32,32,3]).astype(np.float32)/255, mnist_val_y.astype(np.int32)))\n",
        "\n",
        "#Standardizing the test dataset\n",
        "mnist_test_y = mnist_test_y.reshape((-1))\n",
        "mnist_test_x = tf.data.Dataset.from_tensor_slices((mnist_test_x.reshape([-1,32,32,3]).astype(np.float32)/255, mnist_test_y.astype(np.int32)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-4bf893797034>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Standardizing the train dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmnist_train_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist_train_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcifar_train_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_train_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist_train_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Standardizing the validation dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    680\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m     \"\"\"\n\u001b[0;32m--> 682\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   3009\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3010\u001b[0m       batch_dim.assert_is_compatible_with(tensor_shape.Dimension(\n\u001b[0;32m-> 3011\u001b[0;31m           tensor_shape.dimension_value(t.get_shape()[0])))\n\u001b[0m\u001b[1;32m   3012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3013\u001b[0m     variant_tensor = gen_dataset_ops.tensor_slice_dataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       raise ValueError(\"Dimensions %s and %s are not compatible\" %\n\u001b[0;32m--> 280\u001b[0;31m                        (self, other))\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmerge_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Dimensions 12250 and 48000 are not compatible"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF4vaIhvMqHu",
        "colab_type": "text"
      },
      "source": [
        "## Task 1: Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozBleTfLMyVU",
        "colab_type": "text"
      },
      "source": [
        "* Motivation    : Analyze the affect of different Optimizers on the training behaviour\n",
        "  \n",
        " * Optimizer 1: Adadelta,\n",
        " * Optimizer 2: Adagrad\n",
        " * Optimizer 3: Adam)\n",
        " * Optimizer 4: Adamax,\n",
        " * Optimizer 5: Ftrl,\n",
        " * Optimizer 6: Nadam,\n",
        " * Optimizer 7: SGD\n",
        "\n",
        "* Dataset       :\n",
        " * Cifar10, \n",
        " *MNIST Fashion\n",
        "\n",
        "* Architecture  :\n",
        " * Basic Convolutional Neural Network\n",
        " * Resnet\n",
        " * InceptionNet\n",
        " * Densely Connected Convolutional Neural Network\n",
        "\n",
        "* Comparision Criteria : Training Curve\n",
        " * Steps unitll convergence\n",
        " * Training Accuracy \n",
        " * Loss\n",
        " * Stability of Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8S35KUbKKcG",
        "colab_type": "text"
      },
      "source": [
        "### TASK 1.1 BASIC Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRhhL7rUGbpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQgtH-faBiGW",
        "colab_type": "text"
      },
      "source": [
        "## Task 2 : Reqularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk7jZNgOZo96",
        "colab_type": "text"
      },
      "source": [
        "* Motivation: Investigate how different Regularization techniques affect the training behavior.\n",
        "\n",
        "* Dataset : Cifar10\n",
        "\n",
        "* Architecture : Best model from Task1's architecture\n",
        "\n",
        "* Regularization techniques:\n",
        " * L1, L2 regularizer\n",
        " * Dropouts\n",
        " * Adding noise\n",
        "\n",
        "* Comparision Criteria: Training Curves\n",
        " * Steps still convergence\n",
        " * Training accuracy untill convergence\n",
        " * Stability of training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCk4ei_phKg2",
        "colab_type": "text"
      },
      "source": [
        "## Task 3: Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr94NiqF6-E2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}